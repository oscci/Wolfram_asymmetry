---
title: "R Notebook for GAM"
output: html_notebook
---
NB! Based on GAM_Laterality project. 
Updated by DVMB 31 Jan 2024



Based on updated version that creates output file with different tasks on different rows, i.e., long format.

```{r loadpackages}
library(osfr)
library(utils)
require(dplyr)
require(tidyverse)
require(boot)
require(ggpubr)
library(psych)
#library(nlme)
library(plm)
require(mgcv)
library(blandr)
library(gratia) #tools to extend plotting and analysis of mgcv models by Pederson et al
library(performance)
library(GGally)
library(here)
require(nlme) #for glsControl
require(reshape2)
require(stringr)
```

The following is the main function to run the analysis. The function does the following in order:

PART 1:  

Script takes a raw .exp datafile and preprocesses it ready for GAM analysis:  
- It downsamples from 100 Hz to 25 Hz
- It identifies markers in the marker channel - these define epoch timings
- It creates a box car function showing when the task was ON or OFF  
- It normalises the L and R fTCD signals to a mean of 100 by dividing by respective channel mean. This adjusts for any constant differences between L and R that may relate to angle of insonation.
- It performs heart beat integration (identified regular peaks in waveform and averages over the peak-to-peak interval). This removes a major, systematic source of variability that is of no interest.
- It creates a channel corresponding to the epoch number
- It performs baseline correction for each epoch separately for L and R by subtracting the mean value during the baseline period from the signal across the whole epoch. This ensures L and R are equated at the start of the epoch.
- It saves the processed data into a .csv file  
- The mean L and R plots after baseline correction are saved in GAM_figs_baselined 

PART 2:  
- Downsamples to have one timepoint for each heartbeat (to avoid v high autocorrelation)
- Converts file to long form with L and R stacked on top of each other

PART 3:  

- runs the GLMs  
- saves the parameter estimates to data.frame.  
- plots epoch-level estimates from model



```{r readfiles}
#Here we make a list of raw files

usebdata <- 0 #obsolete but retained to avoid errors! If 1, then use baselined data

rawdatafiles <- "/Users/dorothybishop/Rprojects/COLA_analysis_2022/ftcd_raw_data"
ftcdlong<-as.data.frame(list.files(rawdatafiles))
names(ftcdlong)<-'Filename'
ftcdlong$ID<-substr(ftcdlong$Filename,6,12)
ftcdlong$task<-substr(ftcdlong$Filename,14,15)


dataout <- here("processed_data")


tasks <- c('WG','SG','PD','WC','SC','SD')
taskIDs<-LETTERS[1:6]


timings <- as.data.frame(read_csv(here("timings.csv")))
samplingrate <- 25 # Sampling rate *after* downsampling. Raw data is 100Hz, we take 1 in every 4 samples

```

```{r createinclusionlistfile}
makeinclusions <- 1 #file specifying included trials - taken from original ftcd_data file. This is now created in long format. Once it has been made, this step can be skipped by setting makeinclusions flag to 0

if(makeinclusions == 1){
myfolder<-"/Users/dorothybishop/Rprojects/COLA_analysis_2022/data/"
origdata<-read_csv(paste0(myfolder,'ftcd_data.csv'))
inclusionlist<-ftcdlong

inclusionlist$taskcode<-NA
for (r in 1:nrow(inclusionlist)){
  thistask<-inclusionlist[r,3]
  inclusionlist[r,4]<-LETTERS[match(thistask,tasks)]
}
nucols<-paste0('X',1:18)
for (c in 1:18){
inclusionlist[,(c+4)]<-NA
names(inclusionlist)[(c+4)]<-paste0('X',c)
}

for (r in 1:nrow(inclusionlist)){
  thisid<-inclusionlist[r,2]
  oldrow<-match(thisid,origdata$Gorilla_ID)
  thistask<-inclusionlist$taskcode[r]
  startblock<-which(names(origdata)==paste0(thistask,'1'))
  inclusionlist[r,5:22]<-origdata[oldrow,startblock:(startblock+17)]
}
            
write.csv(inclusionlist,here('inclusionsfile.csv'),row.names=F)}

inclusionlist<-read.csv('inclusionsfile.csv')
```


FUNCTIONS DEFINED HERE  - run these prior to main script.
```{r numformat,echo=F}
#Format numbers so they have same n decimal places, even if zero at end
#This returns a string

numformat=function(mynum,ndecimals){
  newnum <- format(round(mynum,ndecimals),nsmall=ndecimals)
  return(newnum)
}

```

```{r corformat,echo=F}
#Format correlation so they have same n decimal places,and no initial zero

corformat=function(mynum,ndecimals){
  newnum <- format(round(mynum,ndecimals),nsmall=ndecimals)
  neg<-''
  if(mynum<0){
    neg<-'-'
    mynum<-substring(newnum,2)} #strip off minus sign - will put it back later
  newnum<-substring(newnum,2) #strip off initial zero
  newnum<-paste0(neg,newnum)
  
  return(newnum)
}

```

```{r meanCI}
#create string with mean and 95% CI in brackets
meanCI <- function(myvar,ndec){
  mymean<-mean(myvar,na.rm=T)
  se<-sd(myvar,na.rm=T)/sqrt(length(myvar))
  CIlow <- numformat(mymean-1.96*se,ndec)
  CIhi <- numformat(mymean+1.96*se,ndec)
  nunum <- paste0(numformat(mymean,ndec), " [",CIlow,', ',CIhi,']')
  return(nunum)
}


```

###########################################
# STEP 1: based on original fTCD analysis #
#                                         #
# Created by z.woodhead 30th July 2019    #
# Edited  by z. woodhead 3rd Oct 2019  
# Edited by DVMB June-July 2022           #
###########################################

```{r preprocessing}  

#This does the steps listed above as part 1, and returns the processed file with stimulus intervals and POI marked, heartbeat correction done, as well as baseline corrected values (though latter not used for GLM). It also returns a list which gives the timings for the heartbeats in the signal (peaklist), which is used later on when reducing the data to one value per heartbeat. IN addition, it updates the summary.data file to include the recomputed LIs using the averaging method on baselined data. (These should agree with values in the downloaded WGfile, though there may be minor discrepancies, as those were computed in 2016 using a different script)

ftcd_preprocess<-function(mypath,filename1,inclusions,mytimings,ntrials,samplingrate,summary.data,mysub){ #this just runs one person at a time 
  
  
  ## Set parameters
  
  heartratemax <- 125 # Used to ensure that correspondence between peaks/heartbeats is in realistic range
 samples<-mytimings
 samples[2:(ncol(mytimings)-1)]<-mytimings[2:(ncol(mytimings)-1)]*samplingrate
 samples<-samples[,-1]
  
  saveepochedavg<-0 #FLAG: keep this at zero unless you want to save the averaged baselined files (the ones used for ftcd LI computation by averaging)
  # If this is set to 1, you get a plot of the L and R channel means after baseline correction in GLM_figs_baselined
  
  
  
  print(paste0(j,": ",filename1)) #show the file on screen to monitor progress
  ## Read in raw data
  
  mydata<-read.table(paste0(mypath,"/",filename1), skip = 6,  header =FALSE, sep ='\t')
  
   wantcols = c(2,3,4,9) #sec, L, R,marker #select columns of interest to put in shortdat
 #in some systems, there are more than 9 channels! This will relate to the settings of the Doppler recording. In this case need col 11 for marker
  if(ncol(mydata)>12){wantcols<-c(2,3,4,11)}
   if(ncol(mydata)<9){wantcols<-c(2,3,4,7)} #and some have trigger in 7!
  shortdat = data.frame(mydata[,wantcols])
  # shortdat$V9 <- stats::filter(shortdat$V9, filter = rep(1/4, 4), sides = 2)
  # #line above applies moving average filter to avoid losing data from brief markers when downsampling
  rawdata = dplyr::filter(shortdat, dplyr::row_number() %% 4 == 0) # downsample from 100  Hz to 25 Hz by taking every 4th point (nb we still see markers, because duration of marker signal is much longer than 4 timepoints)
  allpts = nrow(rawdata) # total N points in long file
  rawdata[,1] = (seq(from=1,to=allpts*4,by=4)-1)/100 #create 1st column which is time in seconds from start
  colnames(rawdata) = c("sec","L","R","marker")
  
  includeepochs<-inclusions #0 or 1 for each trial - trials marked 0 excluded for signal dropout or failure to do task
  excludeepochs<-which(includeepochs!=1) #a list of trials that will be excluded from computations (these determined from original published study)
  #-1 code used for those excluded from inspection of trial during initial analysis
  
  #----------------------------------------------------------
  ## Find markers; place where 'marker' column goes from low to high value
  # Marker channel shows some fluctuation but massive increase when marker is on so easy to detect
  
  mylen = nrow(rawdata); # Number of timepoints in filtered data (rawdata)
  markerplus = c(rawdata$marker[1] ,rawdata$marker); # create vectors with offset of one
  markerchan = c(rawdata$marker,0); 
  markersub = markerchan - markerplus; # start of marker indicated by large difference between consecutive data points
  meanmarker <- mean(rawdata$marker,na.rm=T) # We will identify big changes in marker value that are > 4 sds [7 Nov 2023, revised to allow for NA values]
  markersize <- meanmarker+4*sd(rawdata$marker,na.rm=T) #[7 Nov 2023, revised to allow for NA values]
  origmarkerlist = which(markersub>markersize)
  norigmarkers = length(origmarkerlist) #This should match the N markers on origdata
  nmarker<-norigmarkers
  #boxcar function for generation and reporting periods: will be used when defining gamma functions
  rawdata$stim1_on <- 0 #for generation period - default to zero; 1 when on
   
 for (m in 1:norigmarkers){
    rawdata$stim1_on[(origmarkerlist[m]+samples$stim1start[1]):(origmarkerlist[m]+samples$stim1end[1])] <- 1
   
  #if first marker is less than 300, pad the initial part of file by repeating initial values
  #These do not affect computations for standard method, but prevent crashes later on.
  
  firstm <-origmarkerlist[1]
  if (firstm<300){
    rawdata<-rbind(rawdata,rawdata[1:(301-firstm),])
    origmarkerlist = origmarkerlist+(301-firstm)
  }
 } 
  
  #---------------------------------------------------------- 
  # Identify raw datapoints below .0001 quartile (dropout_points) and above .9999 quartile (spike_points)
  # (In our analysis we'd usually check these visually, as this criterion can miss them, but this allows us to take out extreme artefacts - usually v rare by definition)
  
  dropout_points <- c(which(rawdata$L < quantile(rawdata$L, .0001)), 
                      which(rawdata$R < quantile(rawdata$R, .0001)))
  
  spike_points <- c(which(rawdata$L > quantile(rawdata$L, .9999)),
                    which(rawdata$R > quantile(rawdata$R, .9999)))
  
  if(length(dropout_points)==0){dropout_points <-1 } #kludge added because otherwise if there are no dropout or spike points, meanL and meanR are Nan! Losing one point is not going to have any effect here
  #----------------------------------------------------------
  # Data normalisation: ensures L and R means are the same overall. NB does NOT use variance in this computation
  
  meanL=mean(rawdata$L[-c(dropout_points,spike_points)],na.rm=T)
  meanR=mean(rawdata$R[-c(dropout_points,spike_points)],na.rm=T)
  rawdata$normal_L=rawdata$L/meanL * 100 
  rawdata$normal_R=rawdata$R/meanR * 100
  #For the dropout and spiking timepoints, substitute the mean (added by DB)
  rawdata$normal_L[c(dropout_points,spike_points)]<-meanL
  rawdata$normal_R[c(dropout_points,spike_points)]<-meanR
  #----------------------------------------------------------
  # Heartbeat integration: The heartbeat is the dominant signal in the waveform - v obvious rhythmic pulsing. We look for peaks in the signal that correspond to heart beat
  peaklist=numeric(0)
  pdiff=numeric(0)
  badp=numeric(0)
  
  # Look through every sample from 6, to number of samples minus 6
  for(i in seq(6,mylen-6))
  {if(
    (rawdata$L[i] > rawdata$L[i-5])
    & (rawdata$L[i] > rawdata$L[i-4])
    & (rawdata$L[i] > rawdata$L[i-3])
    & (rawdata$L[i] > rawdata$L[i-2])
    & (rawdata$L[i] > rawdata$L[i-1])
    & (rawdata$L[i] > rawdata$L[i+1])
    & (rawdata$L[i] > rawdata$L[i+2])
    & (rawdata$L[i] > rawdata$L[i+3])
    & (rawdata$L[i]> rawdata$L[i+4])
    & (rawdata$L[i]> rawdata$L[i+5]))
  {peaklist=c(peaklist,i)
  }
  }
  
  # Check that the heartbeats are spaced by far enough!
  peakdiffmin = 60/heartratemax * samplingrate
  pdiff <- peaklist[2:length(peaklist)]-peaklist[1:(length(peaklist)-1)] # pdiff is a list of the number of samples between peaks
  badp<-which(pdiff<peakdiffmin) # badp is a list of the pdiff values that are less than peakdiffmin
  if (length(badp) != 0)
  {peaklist<-peaklist[-(badp+1)] # update peaklist, removing peaks identified by badp
  }
  #print(dim(rawdata))
  #print(peaklist)
  # Do heart beat integration
  peakn=length(peaklist)
  rawdata$heartbeatcorrected_L <- 0
  rawdata$heartbeatcorrected_R <- 0 
  for (p in 1:(peakn-1))
  {myrange=seq(peaklist[p],peaklist[p+1]) # the indices where the heartbeat will be replaced
  thisheart_L=mean(rawdata$normal_L[myrange]) # the new values that will be replaced
  thisheart_R=mean(rawdata$normal_R[myrange])
  rawdata$heartbeatcorrected_L[peaklist[p] : peaklist[p+1]]=thisheart_L
  rawdata$heartbeatcorrected_R[peaklist[p] : peaklist[p+1]]=thisheart_R
  if (p==1){
    rawdata$heartbeatcorrected_L[1:peaklist[p]] <- thisheart_L
    rawdata$heartbeatcorrected_R[1:peaklist[p]] <- thisheart_R
  }
  if (p==peakn-1){
    rawdata$heartbeatcorrected_L[peaklist[p] : mylen] <- thisheart_L
    rawdata$heartbeatcorrected_R[peaklist[p] : mylen] <- thisheart_R
  }
  }
  
  #To inspect a portion of the data can  set seeprocessed to 1 which will run this bit:
  seeprocessed<-0 #nb usually set seeprocessed to zero.
  if(seeprocessed==1){
    plot(rawdata$sec[1:5000],rawdata$heartbeatcorrected_L[1:5000],type='l',col='blue')
    lines(rawdata$sec[1:5000],rawdata$heartbeatcorrected_R[1:5000],type='l',col='red')
    lines(rawdata$sec[1:5000],120*rawdata$stim1_on[1:5000]) #marker superimposed as block
  }
  #--------------------------------------------------------------------------------------------
  # Identify extreme datapoints with values below 60 and above 140
  
  extreme_points <- c(which(rawdata$heartbeatcorrected_L < 60),
                      which(rawdata$heartbeatcorrected_L > 140),
                      which(rawdata$heartbeatcorrected_R < 60),
                      which(rawdata$heartbeatcorrected_R > 140))
  
  #remove outlier cases
  rawdata$heartbeatcorrected_L[extreme_points]<-NA
  rawdata$heartbeatcorrected_R[extreme_points]<-NA
  
  # EPOCHING
  #initialise columns showing epoch and time relative to epoch start for each epoch (see below)
  rawdata$epoch<-NA #initialise new column
  rawdata$relativetime<-NA #initialise new column
  rawdata$task<-NA #this will specify whether sentence, word or list generation trial
  rawdata$stim1_on<-0

  
  #In previous versions, did this in an array (epoch as one dimension) for efficiency, but here done sequentially as easier to keep track.
  nmarker<-length(origmarkerlist)
  
  for (i in 1:nmarker){
    epochrange<-(origmarkerlist[i]+samples$epochstart):(origmarkerlist[i]+samples$epochend)
    #remove values beyond end of time range
    w<-which(epochrange>nrow(rawdata))
    if(length(w)>0){epochrange<-epochrange[-w]}
    
    rawdata$epoch[epochrange]<-i
    #rawdata$task[epochrange]<-task_order[i] #column included if more than one task to show which task (dataset3)
    rawdata$relativetime[epochrange]<- seq(from=mytimings$epochstart,  by=.04,length.out=length(epochrange))        
  }
  
  stim1time<-intersect(which(rawdata$relativetime>=mytimings$stim1start),which(rawdata$relativetime<=mytimings$stim1end))
  rawdata$stim1_on[stim1time]<-1

  
  rawdatax<-rawdata #retain original with all values
  w<-which(is.na(rawdata$relativetime))
  rawdata<-rawdata[-w,] #pruned to include only epochs, i.e. those with values for relativetime
  
  #add specification of  POI; defaults to 0; 1 for values within POI window
  rawdata$POI<-0
  w<-intersect(which(rawdata$relativetime>=mytimings$POIstart),which(rawdata$relativetime<mytimings$POIend))
  rawdata$POI[w]<-1
  
  # Baseline correction - (added to rawdata by DB ).
  
  rawdata$Lbaselined<-NA
  rawdata$Rbaselined<-NA
  
  #Exclude epochs marked for trial exclusion in the original summary.data fil
  w<-which(rawdata$epoch %in% excludeepochs)
  if(length(w)>0){
    rawdata$heartbeatcorrected_L[w]<-NA
    rawdata$heartbeatcorrected_R[w]<-NA
  }
  #
  for (m in 1:nmarker){
    mypoints<-which(rawdata$epoch==m)
    temp<-intersect(mypoints,which(rawdata$relativetime >= mytimings$basestart))
    temp1<-intersect(temp,which(rawdata$relativetime<mytimings$baseend))
    meanL<-mean(rawdata$heartbeatcorrected_L[temp1],na.rm=T)
    meanR<-mean(rawdata$heartbeatcorrected_R[temp1],na.rm=T)
    rawdata$Lbaselined[mypoints]<-100+rawdata$heartbeatcorrected_L[mypoints]-meanL
    rawdata$Rbaselined[mypoints]<-100+rawdata$heartbeatcorrected_R[mypoints]-meanR
  }
  
  # Average over trials by task
  
  aggL <- aggregate(rawdata$Lbaselined,by=list(rawdata$relativetime),FUN='mean',na.rm=T)
  aggR <- aggregate(rawdata$Rbaselined,by=list(rawdata$relativetime),FUN='mean',na.rm=T)
  myepoched_average<-aggL
  myepoched_average<-cbind(myepoched_average,aggR[,2])   #needs modifying if there is also a task column (if more than one type of marker in this task)
  colnames(myepoched_average)<-c('secs','Lmean','Rmean') #needs modifying if also task column
  
  myepoched_average$LRdiff <- myepoched_average$Lmean - myepoched_average$Rmean
  
  
  #Compute means and store in original file to check against saved
  POIs<-rawdata[rawdata$POI==1,]
  POIs$diff<- POIs$Lbaselined-POIs$Rbaselined
  aggmeans<-aggregate(POIs$diff,by=list(POIs$epoch),FUN=mean,na.rm=T) #recompute the LI #use aggregate if there are several tasks
  
  summary.data$LI.mean[j]<-mean(aggmeans$x,na.rm=T)
  summary.data$LI.N[j]<-length(which(!is.nan(aggmeans$x)))
  summary.data$LI.se[j]<-sd(aggmeans$x,na.rm=T)/sqrt(summary.data$LI.N[j])
  
  # # Plot myepoched_average
  
  
  filepath<-here("GAMplots")
  
  longepoched<-rbind(myepoched_average,myepoched_average)
  myrange<-1:nrow(myepoched_average)
  
  longepoched$Rmean[myrange]<-longepoched$Lmean[myrange]
  longepoched$Lmean<-'Right'
  longepoched$Lmean[myrange]<-'Left'
  colnames(longepoched)<-c('time','Side','CBV','diff')
  longepoched$Side<-as.factor(longepoched$Side)
  
  
  if(saveepochedavg==1){
    filepath<-here("GAMplots")
    filename2<-paste0(filepath,'/',filename1,'.png')
    g1<-ggplot(data=longepoched, aes(x=time, y=CBV, group=Side)) +
      geom_line(aes(color=Side))+
      ggtitle(filename1)
    ggsave(filename2,g1,width = 6, height = 4)
  }
  
  
  return(list(rawdata,peaklist,summary.data)) 
}


```



```{r make-longdata}
makelongdata<-function(rawdata,peaklist){
  # First downsample to one point per heartbeat. Then convert to longdata
  shortdata<-rawdata[peaklist,] #data reduction
  w<-which(is.na(shortdata$epoch)) #remove rows with missing data
  if(length(w)>0){
    shortdata<-shortdata[-w,]
  }
  
  #create long form 
  longdata<-rbind(shortdata,shortdata) #stack rows for L and R on top of each other
  
  range1<-1:nrow(shortdata)
  longdata$heartbeatcorrected_R[range1]<-longdata$heartbeatcorrected_L[range1] #put data from L in first range
  w<-which(colnames(longdata)=='heartbeatcorrected_R')
  colnames(longdata)[w]<-'hbcorrected'
  longdata$Rbaselined[range1]<-longdata$Lbaselined[range1]
  w<-which(colnames(longdata)=='Rbaselined')
  colnames(longdata)[w]<-'baselined'
  longdata$R<-'right'
  longdata$R[range1]<-'left'
  w<-which(colnames(longdata)=='R')
  colnames(longdata)[w]<-'side'
  
  w<-which(colnames(longdata) %in% c('L','normal_L','normal_R','heartbeatcorrected_L','Lbaselined')) #remove these
  longdata<-longdata[,-w]
  longdata$y <- longdata$hbcorrected
  if(usebdata==1){
    longdata$y <- longdata$baselined  #not currently used, but was used in testing the script
  }
  longdata$sidef<-as.factor(longdata$side)
  levels(longdata$sidef)<-c(1,-1)
  return(longdata)
}  
```





```{r modelfit.save}
modelfit<- function(longdata,summary.data,glm.method){
  # set optimisation parameters 
  glsControl(optimMethod = "L-BFGS-B",maxIter = 100)
 
  #Based on earlier script that contrasted different methods. Here only method 3 (GAM2) 

    longdata$epoch<-as.factor(longdata$epoch) #instead of time, use relativetime and epoch (latter as factor).
    myfit <- gam(y~s(sec)+s(relativetime)+s(relativetime,by=epoch)+POI+side+POI*side,data=longdata)

   col1<-which(colnames(summary.data)=="GAM2_param1")
    s<-summary(myfit)
    sp<-s$p.pv #pvalues of coefficients
    ncoeffs<-length(sp)
    pinteract<-round(sp[ncoeffs],2) #p value of interaction term (last coefficient)
    summary.data[j,col1:(col1+ncoeffs-1)] <- anova(myfit)$'p.coeff'  #parameter coefficient (not pvalue!)
    
    summary.data[j,(col1+ncoeffs)]<-s$se[ncoeffs]
    summary.data[j,(col1+ncoeffs+1)]<-pinteract
    summary.data[j,(col1+ncoeffs+2)]<-summary(myfit)$r.sq 
    summary.data[j,(col1+ncoeffs+3)]<- round(AIC(myfit),1) #
   summary.data[j,(col1+ncoeffs+4)]<- round(BIC(myfit),1)
    summary.data$GAM2_LI.est[j]<- -1*summary.data$GAM2_LI.est[j] #need to reverse polarity of LI estimate!
  allreturn <-list(summary.data,myfit)
  return(allreturn)
}
```


```{r LIcat}
#use SE to compute CI to divide cases according to categorical laterality
#count how many are R, bilateral and LI
LIcat <- function(summary.data,glm.method,col1,col2){
  #check if columns exist for lat and CIs - if not create them and initialise
  colname<-paste0(LETTERS[glm.method],'_lat')
  startn<-which(colnames(summary.data)==colname)
  if(length(startn)==0)
  {n<-ncol(summary.data)
  summary.data[,(n+1):(n+3)]<-NA
  colnames(summary.data)[(n+1):(n+3)]<-c(paste0(LETTERS[glm.method],'_lat'),
                                         paste0(LETTERS[glm.method],'_lowCI'),
                                         paste0(LETTERS[glm.method],'_highCI'))
  startn<-n+1 #start writing new values to next col after the previous col total
  }
  
  #now compute the confidence limits for these estimates using the SE from col 2
  LIlowCI <- col1-1.96*col2
  LIhighCI <- col1+1.96*col2
  
  #LIlat is -1, 0 or 1 for R, bilateral or L lateralised: 
  #initialise this with zero
  LIlat<-rep(0,length(LIlowCI))
  w<-which(LIlowCI > 0)
  LIlat[w]<-1 #if lower CI is greater than zero, then left-lateralised
  w<-which(LIhighCI<0)
  LIlat[w]<--1 #if higher CI is less than zero, then right-lateralised
  
  t<-table(LIlat) 
  proptab<-numformat(100*t/sum(t),1) #make a little table with proportions of each type 
  
  
  #write these values to summarydata.
  summary.data[,startn]<-LIlat
  summary.data[,(startn+1)]<-LIlowCI
  summary.data[,(startn+2)]<-LIhighCI
  return(list(proptab,summary.data))
}

```
##############################################################
# MAIN ANALYSIS LOOP STARTS HERE
##############################################################

```{r run-analysis}
analysisdone<-1
if(analysisdone==0){

summary.data<-ftcdlong #we'll bolt model fit results onto  results from original 
#but we do also recompute the LI here using the baselined values and averaging over POI


summary.data$LI.mean <-NA #initialise
summary.data$LI.se <-NA
summary.data$LI.N <-NA
summary.data$Npts<-NA #record how many pts in final analysis (need to know epoch dur to translate into effective sampling rate)


addbit<-data.frame(matrix(NA,nrow=nrow(summary.data),ncol=9))

nunames<-c('param1','param2','param3','LI.est','LIest.se','p.interact','R2','AIC','BIC')
colnames(addbit)<-paste0("GAM2_",nunames)
summary.data<-cbind(summary.data,addbit)


glmlist<-c('GAM2')

glmmethods<- 3 #can have a vector here if need be, or can just have, e.g.  model 3 is GAM2

#NB Next line is obsolete but were used in testing models.  For GLM and GAM we stick with unbaselined data
bindex<-c('n') #used to indicate whether baselined when storing data

startj<-1
endj<-nrow(summary.data)

#endj<-startj

#endj=3 #temporary override when testing program. Comment this out later


jrange<-startj:endj


for (j in jrange){
thisfile<-summary.data$Filename[j]
w<-which(inclusionlist$Filename==thisfile)
if(length(w)>0){
ww<-which(names(inclusionlist)=="X1")
inclusions <- inclusionlist[w,ww:(ww+17)] #cols for the individual trials (omits cols 1-4 by starting at column X1)
  #run ftcd_preprocess function before running this chunk, so functions are in memory

  #Need to have folder GAM_figs_baselined



w2<-which(timings$task==summary.data$task[j])
mytimings<-timings[w2,]
ntrials<-mytimings$ntrials
filename1<-summary.data$Filename[j]
mypath<-rawdatafiles
myn<-length(which(inclusions==1))
if(myn > 9){
myreturn<-ftcd_preprocess(mypath,
                            filename1,
                            inclusions,
                            mytimings,
                            ntrials,
                            samplingrate,
                            summary.data,
                            j)
  #NB modified to feature inclusions file, which documents which trials were excluded in original analysis
  
  rawdata<-myreturn[[1]]
  peaklist<-myreturn[[2]]
  summary.data<-myreturn[[3]]
  summary.data$Npts[j]<-length(peaklist) #record how many pts in final analysis 
  
  
  #rawdata<-do.hdr(rawdata,timings,samplingrate) #adds HDR for generation and report periods
  
  npts<-nrow(rawdata)
  
  stim1_length_samples<-(mytimings$stim1end[1]-mytimings$stim1start[1])*samplingrate

  
  longdata <- makelongdata(rawdata,peaklist) 
  
  glm.method <- 3 #GAMplus method
    
    glmreturn <- modelfit(longdata,summary.data,glm.method)
    summary.data<-glmreturn[[1]]
    myfit<-glmreturn[[2]]
    
  
  
  
  # 
  if((j/5)== round(j/5,0)){
    write.csv(summary.data,here('dumpGLMsummary.csv')) #periodically save the file
  }
}
}

#add handedness
summary.data$Rhand <- NA
for (i in 1:nrow(summary.data)){
  thisID<-summary.data$ID[i]
  w<-which(origdata$Gorilla_ID==thisID)
  if(length(w)>0){
  summary.data$Rhand[i]<-origdata$Hand_R[w[1]]
  }
}
write.csv(summary.data,here('processed_data','GAMdone.csv'),row.names=F)
}
}

if(analysisdone==1){
  summary.data<-read.csv(here('processed_data','GAMdone.csv'))
}

```

NEXT SECTION selects data for plotting

```{r selectdata}


df <- summary.data
df$Handed<-"Left-hander"
df$Handed[df$Rhand==1]<-"Right-hander"
df$Handed<-as.factor(df$Handed)
w<-which(is.na(df$LI.mean)) #remove blanks

df<-df[-w,]


df$lowCI<-df$GAM2_LI.est-1.96*df$GAM2_LIest.se
df$hiCI<-df$GAM2_LI.est+1.96*df$GAM2_LIest.se

df$siglat<-0
df$siglat[df$hiCI<0]<--1
df$siglat[df$lowCI>0]<-1
df$mycolor <-2
df$mycolor[df$siglat==0]<-1


nuorder<-order(df$task,df$Rhand,df$GAM2_LI.est)
myf<-df[nuorder,] #put in order of task, then hand then LI

#LIs now in sequential list within task/hand; can create row by order by just resetting to 1 when LI goes down

LIs<-c(-10,myf$GAM2_LI.est)
LIdiff<-LIs[2:length(LIs)]-LIs[1:(length(LIs)-1)]
mychange<-c(1,which(LIdiff<0),nrow(myf) )#specifies start and end of ranges for hand/task
myf$row<- NA
for (c in 1:(length(mychange)-1)){
  thisrange<-mychange[c]:mychange[c+1]
  myf$row[thisrange]<-1:length(thisrange)
}

#change WC to WD and SC to SeD for consistency with Parker et al
w<-which(myf$task=='WC')
myf$task[w]<-'WD'
w<-which(myf$task=='SD')
myf$task[w]<-'SyD'
w<-which(myf$task=='SC')
myf$task[w]<-'SeD'

#make task as ordered factor - this time order is in extent of L lateralisation
myf$task<-factor(myf$task,levels=c('SG','WG','PD','SeD','SyD','WD'))

tasks <- c('SG','WG','PD','SeD','SyD','WD') #revise names to match those used by Parker et al

tasknames<-c("Sentence Generation","Word Generation","Phonological Decision","Sentence Decision","Syntactic Decision","Word Decision")



# Identify outliers - we have already excluded those with < 10 trials
# Apply Hoaglin-Iglewicz formula
myf$exclude==0

 Q3<-quantile(myf$GAM2_LIest.se,.75,na.rm=TRUE)
  Q1<-quantile(myf$GAM2_LIest.se,.25,na.rm=TRUE)
  Qlimit<-Q3+2.2*(Q3-Q1)

  
  # If the SE is too high, exclude the datatrials
  w<-which(myf$GAM2_LIest.se > Qlimit)
  myf<-myf[-w,]

```


```{r doplatestackplot}
mycols<-c('black','red')
g<-ggplot(myf,aes(y=row,x=GAM2_LI.est,col=mycolor))+
  geom_point(size=.5,color=mycols[myf$mycolor])+
  xlab("Laterality index")+
  ylab("Rank order")+
  geom_errorbar(aes(xmin=lowCI, xmax=hiCI),  width=.05,color=mycols[myf$mycolor])+
  geom_vline(xintercept=0, linetype="dotted")
  
g<-g+
  facet_grid(task ~ Handed)+
    theme( axis.text = element_text( size = 12 ),
         axis.text.x = element_text( size = 10 ),
         axis.title = element_text( size = 12),
         title = element_text(size=16),
         legend.position="none",
         # The new stuff
         strip.text = element_text(size = 14))

plotname<-'taskall_plot.png'
ggsave(here("plots",plotname),width=6,height=8)
```

```{r maketable}
#Now make a table showing proportions lateralised plus normality test
propdf<-data.frame(matrix(NA,nrow=12,ncol=6))
names(propdf)<-c('Task','N','R_lat%','bilat%','L_lat%','Shapiro_test')
thisrow<-0
for (t in 1:length(tasks)){
  for (h in 0:1){
    thisrow<-thisrow+1
  propdf[thisrow,1]<-paste0(tasknames[t],": L-handed")
  if(h==1){
      propdf[thisrow,1]<-paste0(tasknames[t],": R-handed")
  }
   w<-intersect(which(myf$task==tasks[t]),which(myf$Rhand==h))
   temptab<-round(100*table(myf$siglat[w])/length(w),1)
   propdf[thisrow,3:5]<-temptab
   propdf[thisrow,2]<-length(w)
   myshap<-shapiro.test(myf$GAM2_LI.est[w])$p.value
   propdf[thisrow,6]<-'<.001'
   if(myshap>.001){
     propdf[thisrow,6]<-round(myshap,3)
   }
  }
}

write.csv(propdf,'Props_hand_task.csv',row.names=F)

```


Add correlation heatmap for tasks

```{r tribit}
#Gets upper (or lower if specified) triangle of a correlation matrix
 get_tri <- function(cormat){
    cormat[upper.tri(cormat,diag=F)]<- NA
    return(cormat)
  }
```

```{r heatmapfunction,echo=F}
#Make a heatmap
#Modified to remove diagonal values
# http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization
makeheatmap <- function(mydf,mycols,corrtype){
cormat <- cor(mydf[,mycols],use="complete.obs",method=corrtype)
melted_cormat <- melt(cormat) #whole correlation matrix in a table
head(melted_cormat)

mytri <- get_tri(cormat)
# Melt the correlation matrix

melted_cormat <- melt(mytri, na.rm = TRUE)

melted_cormat$mycolour<-'NA'  #colour used to set diagonal values to have  frame
melted_cormat$corlabel<-'' #formatting correlation matrix to 2 decimals

#remove diagonals
w<-which(melted_cormat$value==1)
melted_cormat<-melted_cormat[-w,]

for (r in 1:nrow(melted_cormat)){
  if(melted_cormat$Var1[r]==melted_cormat$Var2[r]){
    melted_cormat$mycolour[r]<-'grey'
  }
    melted_cormat$corlabel[r]<-corformat(melted_cormat$value[r],2)
}


# Heatmap

#original heatmap
corrtype<-str_to_sentence(corrtype)
ggheatmap <- ggplot(data = melted_cormat, aes(Var1, Var2, fill = value))+
 geom_tile(color = melted_cormat$mycolour,size=1.5)+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-0.2,1), space = "Lab", 
   name=paste0(corrtype,"\nCorrelation")) +
  theme_linedraw()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
theme(axis.text.y = element_text(
    size = 10, hjust = 1))+
  scale_y_discrete(limits=rev)+ #gets tasks in same order for x and y axis!
 coord_fixed()

#add some formatting
myheatmap <- ggheatmap + 
geom_text(aes(Var1, Var2, label = corlabel), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.5, 0.1),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))

return(myheatmap)}
```

```{r doheatmap}

forwide<-summary.data[,c("ID","task","GAM2_LI.est","LI.mean","Rhand")]
widedat<-reshape(data=forwide,
                 idvar="ID",
                 v.names=c("GAM2_LI.est","LI.mean"),
                 timevar="task",
                 direction="wide")
mydf<-widedat[,c(1,2,3,5,7,9,11,13,4,6,8,10,12,14)]
names(mydf)[3:8]<-c('PD','SeD','SyD','SG','WD','WG') #reshape has put in alpha order
names(mydf)[9:14]<-paste0('LI.mean.',names(mydf)[3:8])
mycols<-c('WG','SG','PD','WD','SeD','SyD')
hm <- makeheatmap(mydf,mycols,corrtype='spearman') #the function does spearman correls

#
mydfr<-mydf[mydf$Rhand==1,]
mycols2<-paste0('LI.mean.',mycols)
hmr <- makeheatmap(mydfr,mycols2,'pearson')

mydfl<-mydf[mydf$Rhand==0,]
hml <- makeheatmap(mydfl,mycols2,'pearson')





```

Why aren't correlations identical those in the published paper?
First check the data from published paper

Should be same as data in origdata.
NB each task has an _exclude column.
We'll exclude all those rows and then do the heatmaps for L and R handers

```{r checkorig}
w<-c(which(origdata$A_exclude==1),which(origdata$B_exclude==1),which(origdata$C_exclude==1),which(origdata$C_exclude==1),which(origdata$D_exclude==1),which(origdata$E_exclude==1),which(origdata$F_exclude==1))
w<-unique(w)
orig2<-origdata[-w,]

#Ah, this gives 211 cases, whereas mydf has 219
oldcols<-paste0(LETTERS[1:6],'_mean_LI')
oldcormatL <- cor(orig2[orig2$Hand_R==0,oldcols],use="complete.obs",method=corrtype)
oldcormatR <- cor(orig2[orig2$Hand_R==1,oldcols],use="complete.obs",method=corrtype)
oldcormatL<-round(oldcormatL,2)

#So these are not identical to published.
#Suggests different subset of cases.
#redo with full origdata 
oldcormatL <- cor(origdata[origdata$Hand_R==0,oldcols],use="complete.obs",method=corrtype)
oldcormatR <- cor(origdata[origdata$Hand_R==1,oldcols],use="complete.obs",method=corrtype)
oldcormatL<-round(oldcormatL,2)
oldcormatR<-round(oldcormatR,2)

#Nope. Still not the same.
#Need to go back to the original script to see if I can reproduce the table.

```

Looking at stackedplates for fMRI

```{r fmridata}

mymirror <- read.csv('fmri/flip_allmasks_tasks_feb2024.csv')
#ID needs pruning
mymirror$ID<-substr(mymirror$ID,1,5)

subcodes<-read.csv("fmri/subject_codes.csv")

#add handedness code to mymirror: need to look up in subcodes to get correct row
mymirror$Rhand<-0
for (i in 1:nrow(mymirror)){
  mysub <- mymirror$ID[i]
  w<-which(subcodes$ID==mysub)
  if(length(w)>0){
  w2 <- which(df$ID==subcodes$Gorilla[w])}
  mymirror$Rhand[i]<-df$Rhand[w2[1]]
}

#add code of 0 or 1 for sig lateralised or not
mymirror$siglat<-0
w<-c(which(mymirror$diff_low_CI>0),which(mymirror$diff_hi_CI<0))
mymirror$siglat[w]<-1
```

```{r doplatestackplotfmri}
mymasks<-c("frontal"  ,  "temporal" ,  "parietal" ,  "cerebellar" )
mycols<-c('black','red')
for (m in 1:length(mymasks)){
  thismask<-mymasks[m]
  myf<-filter(mymirror,mask==thismask)
  myf<-myf[!is.na(myf$Rhand),]
  myf <- myf[order(myf$task,myf$Rhand,myf$meandiff),] #because of missing data need to set rows nonautomatically
  t<-table(myf$Rhand,myf$task) #Ns for each combination
  v<-as.vector(t)
  lv <- length(v)
  rowlist<-vector()
  for (l in 1:lv){
    rowlist<-c(rowlist,1:v[l])
  }
  myf$row<-rowlist

  g<-ggplot(myf,aes(y=row,x=meandiff,col=(1+siglat)))+
  geom_point(size=.5,color=mycols[(1+myf$siglat)])+
  xlab("Mirror laterality index")+
  ylab("Rank order")+
  geom_errorbar(aes(xmin=diff_low_CI, xmax=diff_hi_CI),  width=.05,color=mycols[1+myf$siglat])+
  geom_vline(xintercept=0, linetype="dotted")
  
g<-g+
  facet_grid(task ~ Rhand)+
    theme( axis.text = element_text( size = 12 ),
         axis.text.x = element_text( size = 10 ),
         axis.title = element_text( size = 12),
         title = element_text(size=16),
         legend.position="none",
         # The new stuff
         strip.text = element_text(size = 14))

plotname<-paste0('taskall_',thismask,'.png')
ggsave(here("plots",plotname),width=6,height=8)
}
```


```{r doplatestackplotfmritask}
mytasks<-c("WG"  ,  "SG" ,  "PD" ,  "WC","SC","SD" )
#Can we use letter codes to indicate individuals?
#First try sorting cases for MCA Sentence Generation
mybit <- filter(mymirror,task=='SG',mask=='mca',!is.na(Rhand))

#Why are there some with no handedness?!

mybito<-mybit[order(mybit$Rhand,mybit$meandiff),]
t<-table(mybito$Rhand)
mybito$alpha<-c(letters[t[1]:1],LETTERS[t[2]:1]) #lowercase for L hander and uppercase Rhander = assign in reverse order because they will plot from bottom to top
#mybito now has alpha code for each participant
#use this as a lookup table - add alpha code to my mirror
mymirror$alpha<-NA
for (i in 1:nrow(mybito)){
  w<-which(mymirror$ID==mybito$ID[i])
  mymirror$alpha[w]<-mybito$alpha[i]
}

mycols<-c('black','red')
for (t in 1:length(mytasks)){
  thistask<-mytasks[t]
  myf<-filter(mymirror,task==thistask)
  myf<-myf[!is.na(myf$Rhand),]
  my
  myf <- myf[order(myf$mask,myf$Rhand,myf$meandiff),] #because of missing data need to set rows nonautomatically
  t<-table(myf$Rhand,myf$mask) #Ns for each combination
  v<-as.vector(t)
  lv <- length(v)
  rowlist<-vector()
  for (l in 1:lv){
    rowlist<-c(rowlist,1:v[l])
  }
  myf$row<-rowlist

  g<-ggplot(myf,aes(y=row,x=meandiff,col=(1+siglat)))+
  geom_point(size=2,color=mycols[(1+myf$siglat)])+
  xlab("Mirror laterality index")+
  ylab("Rank order")+
  geom_errorbar(aes(xmin=diff_low_CI, xmax=diff_hi_CI),  width=.05,color=mycols[1+myf$siglat])+
    geom_point(size=2,x=(myf$diff_low_CI-.15),color='black',shape=myf$alpha)+
    #the shape command puts the ID letter to the left of each plate
  geom_vline(xintercept=0, linetype="dotted")
  
g<-g+
  facet_grid(mask ~ Rhand)+
    theme( axis.text = element_text( size = 12 ),
         axis.text.x = element_text( size = 10 ),
         axis.title = element_text( size = 12),
         title = element_text(size=16),
         legend.position="none",
         # The new stuff
         strip.text = element_text(size = 14))

plotname<-paste0('maskall_',thistask,'.png')
ggsave(here("plots",plotname),width=6,height=8)
}
```